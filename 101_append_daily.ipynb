{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "97954d1f-8e2c-4a97-a50f-2cf4598ead85",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Note:\n",
    "Script 100 creates the initial daily table. This script takes successive Mediatel datasets and then appends daily rows to the initial daily table. This way we don't have to deal with the massive stacked Mediatel dataset each time we want to update the daily metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "34693efe-f763-4f47-ae11-1b38721557ea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Steps:\n",
    "1. Load in Mediatel data each day\n",
    "2. Create metric columns\n",
    "3. Summarise\n",
    "4. Load current summary table\n",
    "5. Append to current summary table\n",
    "6. Save over current summary table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ccca1f27-b87b-45ea-bb1f-204b63f598a2",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Load packages"
    }
   },
   "outputs": [],
   "source": [
    "%run ./my_packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "519e2dc7-f5ab-4c25-b02c-828057c51f24",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# By default mediatel dates is a list containing a single date, however, use below if need to catch up on multiple days at a time\n",
    "mediatel_dates = [mediatel_date]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a8021558-1a16-4458-81ce-ee6f659c4823",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%skip\n",
    "# Set `mediatel_date` here if you want to run manually. Otherwise under BAU conditions (i.e. when you don't need to run any past dates) this cell should be skipped via %skip tag.\n",
    "start = '20260218'\n",
    "end = '20260219'\n",
    "mediatel_dates = [\n",
    "    (datetime.strptime(start, \"%Y%m%d\") + timedelta(days=i)).strftime(\"%Y%m%d\") for i in range((datetime.strptime(end, \"%Y%m%d\") - datetime.strptime(start, \"%Y%m%d\")).days + 1)\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "caed19e8-5a7f-4507-a0d1-18949e9c1f1f",
     "showTitle": true,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{\"call_date\":118},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1771195297071}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      },
      "1": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1771550071701}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 1
      }
     },
     "title": "Read in current date Mediatel extract"
    }
   },
   "outputs": [],
   "source": [
    "for mediatel_date in mediatel_dates:\n",
    "    try:\n",
    "        # Read excel file\n",
    "        df = f_LoadLatestMediatel(mediatel_date=mediatel_date, select_cols=select_cols)\n",
    "\n",
    "        # Create metrics\n",
    "        df = f_CreateMetrics(df, timeframe=\"daily\")\n",
    "\n",
    "        # Summarise\n",
    "        df_summary = f_CreateSummary(df, \"daily\")\n",
    "\n",
    "        df_summary = (\n",
    "            df_summary\n",
    "            .with_columns(pl.col(\"call_date\").dt.date().alias(\"call_date\"))\n",
    "            .with_columns(pl.col(\"call_date\").dt.truncate(\"1w\").dt.date().alias(\"call_week\"))\n",
    "            .with_columns(pl.col(\"call_date\").dt.month_start().dt.date().alias(\"call_month\"))\n",
    "            .with_columns(pl.when(pl.col(\"DPD_type\") == \"Pre-Due\").then(7).otherwise(8).alias(\"HC_Spin_Target\"))\n",
    "            .with_columns(pl.lit(5).alias(\"VB_Spin_Target\"))\n",
    "        )\n",
    "\n",
    "        df_summary = df_summary.to_pandas()\n",
    "\n",
    "        # Load current summary table, which will be as of t-2\n",
    "        table_name = \"teams.cashalo.brett_collections_funnel_daily_v2\"\n",
    "        df_current = spark.sql(f'SELECT * FROM {table_name}').toPandas()\n",
    "\n",
    "        # Save it right back down as a back-up copy\n",
    "        spark.sql(f\"DROP TABLE IF EXISTS {table_name}_backup\")\n",
    "        spark_df_backup = spark.createDataFrame(df_current)\n",
    "        spark_df_backup.write.saveAsTable(f\"{table_name}_backup\")\n",
    "\n",
    "        # Append t-1 summary table to current t-2 summary table\n",
    "        df_combined = pd.concat([df_current, df_summary], ignore_index=True)\n",
    "\n",
    "        # Save over current t-2 summary table with new summary table which will be latest as of t-1\n",
    "        spark_df = f_DailyCreateSparkTable(df_combined, table_name=table_name)\n",
    "        spark_df.write.saveAsTable(table_name)\n",
    "\n",
    "        display(spark_df)\n",
    "            \n",
    "    except Exception:\n",
    "        dbutils.notebook.exit(\"Error encountered. Aborting script to prevent overwrite of summary data.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b3c5700e-4040-490d-82f5-dc5e9bca394b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT * FROM teams.cashalo.brett_collections_funnel_daily_v2"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 6533910240727589,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "101_append_daily",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}