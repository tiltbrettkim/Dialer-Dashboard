{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "f86acbe1-71b7-4b52-97d1-7d0201ec5092",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install polars\n",
    "%pip install fastexcel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2e1d56e5-5a29-4572-96ed-f50a04338baa",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Libraries"
    }
   },
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import glob\n",
    "import fastexcel\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import logging\n",
    "import os.path\n",
    "import sys\n",
    "from datetime import datetime, timedelta, time\n",
    "from pathlib import Path\n",
    "from pyspark.sql import SparkSession\n",
    "from typing import Literal\n",
    "from zoneinfo import ZoneInfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "185d40c0-1437-4096-b8d8-fe68da3637c6",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Hyperparameters"
    }
   },
   "outputs": [],
   "source": [
    "# Uncomment below if I ever need to use spark\n",
    "# spark = SparkSession.builder.config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\").getOrCreate()\n",
    "# Example: spark_main = spark.createDataFrame(df.to_arrow()).createOrReplaceTempView(\"spark_main\") to create a spark dataframe from df\n",
    "\n",
    "tz = ZoneInfo('Asia/Singapore')\n",
    "today = datetime.now(tz)\n",
    "\n",
    "mediatel_date = (today - timedelta(days=1))\n",
    "mediatel_date = mediatel_date.strftime(\"%Y%m%d\")\n",
    "\n",
    "excel_epoch = datetime(1899, 12, 30)\n",
    "\n",
    "select_cols = [\n",
    "    \"Queuedescription\",\n",
    "    \"Incomingcalltime\",\n",
    "    \"IVR_Time\",\n",
    "    \"ExitType\",\n",
    "    \"Calltype\",\n",
    "    \"Call_Reason\",\n",
    "    \"Call_Code\",\n",
    "    \"Hierarchy\",\n",
    "    \"Loan_ID\"\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "298ba484-8d93-44a9-9ba6-02bc44a23924",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def f_LoadLatestMediatel(mediatel_date:str, select_cols:list):\n",
    "    mediatel_file = glob.glob(f\"/Workspace/Users/brett.kim@tilt.com/Mediatel Extracts/{mediatel_date}*.xlsx\")\n",
    "    if not mediatel_file:\n",
    "        raise FileNotFoundError(\"No Mediatel extract found\")\n",
    "    latest_file = max(mediatel_file)\n",
    "    df_mediatel = pl.read_excel(latest_file, columns = select_cols).filter(~pl.col(\"Queuedescription\").is_in([\"Inbound\", \"Outbound\"]))\n",
    "\n",
    "    return df_mediatel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "2e0347f8-9ef8-4749-97aa-db59b5c7f3bb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def f_CreateMetrics(df, timeframe=Literal[\"daily\", \"weekly\", \"monthly\"]):\n",
    "    # The difference between daily, weekly, and monthly timeframes is the group by variables for counting unique accounts over each respective timeframe.  \n",
    "    if timeframe == \"daily\":\n",
    "        group_vars = [\"Loan_ID\"] # Do not need DPD type for daily grouping because accounts do not move between DPD groups in the same day.\n",
    "    elif timeframe == \"weekly\":\n",
    "        group_vars = [\"week_num\", \"DPD_type\", \"Loan_ID\"] # For weekly & monthly need to include DPD type because accounts move DPD groups over time.\n",
    "    elif timeframe == \"monthly\":\n",
    "        group_vars =[\"call_month\", \"DPD_type\", \"Loan_ID\"]\n",
    "    else:\n",
    "        print(f\"Invalid timeframe {timeframe} provided. Can only take 'daily', 'weekly', or 'monthly'\")\n",
    "\n",
    "    hc_logic = (\n",
    "        pl.when((pl.col(\"Calltype\") == \"Manual Outgoing\") & (pl.col(\"Call_Code\") == \"CONNECTION FAILED\")).then(0)\n",
    "        .when((pl.col(\"Calltype\") == \"Outbound predictive\") & (pl.col(\"Call_Code\") == \"CONNECTION FAILED\")).then(1)\n",
    "        .when(((pl.col(\"call_time_of_day\") == pl.lit(\"Daytime\")) & (pl.col(\"Hierarchy\").is_between(1, 12)))).then(1)\n",
    "        .otherwise(0)\n",
    "        .alias(\"HC_Connect\")\n",
    "    )\n",
    "\n",
    "    # Create metrics\n",
    "    df = (\n",
    "        df\n",
    "        # Group Queuedescription into pre-due and M1 buckets\n",
    "        .with_columns(pl.when(pl.col(\"Queuedescription\").str.contains(\"M1\"))\n",
    "                        .then(pl.lit(\"M1\"))\n",
    "                        .otherwise(pl.lit(\"Pre-Due\"))\n",
    "                        .alias(\"DPD_type\"))\n",
    "        # Convert call time into a date and time columns\n",
    "        .with_columns((pl.col(\"Incomingcalltime\").cast(pl.Float64)\n",
    "                        .map_elements(lambda x: excel_epoch + timedelta(days=x) if x is not None else None).alias(\"incoming_calltime_converted\")))\n",
    "        .with_columns(pl.col(\"incoming_calltime_converted\")\n",
    "                        .map_elements(lambda x: x.date() if x is not None else None).alias(\"call_date\"))\n",
    "        .with_columns(pl.col(\"incoming_calltime_converted\")\n",
    "                        .map_elements(lambda x: x.time() if x is not None else None).alias(\"call_time\"))\n",
    "        .with_columns(pl.col(\"IVR_Time\").str.strptime(pl.Time, \"%H:%M:%S\").alias(\"ivr_time_converted\"))\n",
    "        # Bucket call types into Daytime (8am - 5pm) and Afterhours (5pm - 9pm)\n",
    "        .with_columns(pl.when((pl.col(\"call_time\") >= time(8, 0)) & \n",
    "                                (pl.col(\"call_time\") < time(17, 0)))\n",
    "                        .then(pl.lit(\"Daytime\"))\n",
    "                        .otherwise(pl.lit(\"Afterhours\"))\n",
    "                        .alias(\"call_time_of_day\"))\n",
    "        # Create columns for customer connect funnel by creating columns for HC, IVR, and VB calls\n",
    "        # HC Connect\n",
    "        .with_columns(hc_logic.alias(\"HC_Connect\"))\n",
    "        # Split out HC Connects into Agent connects, and various stages of drop-offs\n",
    "        .with_columns(pl.when((hc_logic == 1) & (pl.col(\"Hierarchy\") == 8))\n",
    "                        .then(1)\n",
    "                        .otherwise(0)\n",
    "                        .alias(\"HC_Connect_DropOff\"))\n",
    "        .with_columns(pl.when((hc_logic == 1) & (pl.col(\"Hierarchy\").is_between(9, 10)))\n",
    "                        .then(1)\n",
    "                        .otherwise(0)\n",
    "                        .alias(\"HC_Connect_Voicemail\"))\n",
    "        .with_columns(pl.when((hc_logic == 1) & (pl.col(\"Hierarchy\").is_between(11, 12)))\n",
    "                        .then(1)\n",
    "                        .otherwise(0)\n",
    "                        .alias(\"HC_Connect_Other\"))\n",
    "        # Agent Connect is are calls where agents converse with agents\n",
    "        .with_columns(pl.when((hc_logic == 1) & (pl.col(\"Hierarchy\") <= 7))\n",
    "                        .then(1)\n",
    "                        .otherwise(0)\n",
    "                        .alias(\"HC_Connect_AgentConnect\"))\n",
    "        # Granular breakdown of Agent Connects\n",
    "        .with_columns(pl.when((hc_logic == 1) & (pl.col(\"Hierarchy\") == 1))\n",
    "                        .then(1)\n",
    "                        .otherwise(0)\n",
    "                        .alias(\"AC_PTP\"))\n",
    "        .with_columns(pl.when((hc_logic == 1) & (pl.col(\"Hierarchy\") == 2))\n",
    "                        .then(1)\n",
    "                        .otherwise(0)\n",
    "                        .alias(\"AC_NO_PTP\"))\n",
    "        .with_columns(pl.when((hc_logic == 1) & (pl.col(\"Hierarchy\") == 3))\n",
    "                        .then(1)\n",
    "                        .otherwise(0)\n",
    "                        .alias(\"AC_Paid_Loan\"))\n",
    "        .with_columns(pl.when((hc_logic == 1) & (pl.col(\"Hierarchy\") == 4))\n",
    "                        .then(1)\n",
    "                        .otherwise(0)\n",
    "                        .alias(\"AC_DNC\"))\n",
    "        .with_columns(pl.when((hc_logic == 1) & (pl.col(\"Hierarchy\") == 5))\n",
    "                        .then(1)\n",
    "                        .otherwise(0)\n",
    "                        .alias(\"AC_Call_Back\"))\n",
    "        .with_columns(pl.when((hc_logic == 1) & (pl.col(\"Hierarchy\") == 6))\n",
    "                        .then(1)\n",
    "                        .otherwise(0)\n",
    "                        .alias(\"AC_Hang_Up\"))\n",
    "        .with_columns(pl.when((hc_logic == 1) & (pl.col(\"Hierarchy\") == 7))\n",
    "                        .then(1)\n",
    "                        .otherwise(0)\n",
    "                        .alias(\"AC_PTP_FFUP\"))\n",
    "        # IVR Connect is where a customer picks up a dial, and is put in IVR. This is broken up into two instances:\n",
    "        # In Queue is when dialled, customer picked up, but did not get through to agent. \n",
    "        # Ring Agent is when dialled, customer picked up, and was about to get transferred to agent, but failed in the process\n",
    "        # In either case it's a partial fail since a customer did not connect to an agent.\n",
    "        # When an IVR successfully connects to an agent, it will be counted under one of the HC connections.\n",
    "        .with_columns(pl.when((pl.col(\"ExitType\").is_in(pl.lit([\"In Queue\", \"Ring Agent\"]))) & \n",
    "                                (pl.col(\"Call_Reason\") == pl.lit(\"Closed waiting\")) &\n",
    "                                (pl.col(\"call_time_of_day\") == pl.lit(\"Daytime\")))\n",
    "                        .then(1)\n",
    "                        .otherwise(0)\n",
    "                        .alias(\"IVR_Connect\"))\n",
    "        # VB Connect is like IVR but after hours. \n",
    "        # We have a minimum 3 second call time for a Voice Blaster to be considered as a sucessful connection.\n",
    "        # Including 30 January 2026 and onward, new calling strategy means not all accounts receive human calls, these are the \"COL VB Low Risk Predue -5 to -1\" and \"COL VB Med Risk Predue -5 to -4\" cohorts.\n",
    "        .with_columns(pl.when((pl.col(\"ExitType\") == pl.lit(\"In IVR\")) & \n",
    "                                (pl.col(\"Call_Reason\") == pl.lit(\"Closed In Script\")) &\n",
    "                                ((pl.col(\"call_time_of_day\") == pl.lit(\"Afterhours\")) | (pl.col(\"Queuedescription\").is_in([\"COL VB Low Risk Predue -5 to -1\", \"COL VB Med Risk Predue -5 to -4\"]))) & \n",
    "                                (pl.col(\"ivr_time_converted\").dt.second() >= 3))\n",
    "                        .then(1)\n",
    "                        .otherwise(0)\n",
    "                        .alias(\"VB_Connect\"))\n",
    "        # Create columns for processed accounts & PTP specific counts\n",
    "        .with_columns(pl.when((pl.col(\"Hierarchy\") >= 1) &\n",
    "                                (pl.col(\"Hierarchy\") <= 4))\n",
    "                        .then(1)\n",
    "                        .otherwise(0)\n",
    "                        .alias(\"Processed_account\"))\n",
    "        .with_columns(pl.when(pl.col(\"Hierarchy\") == 1).then(1).otherwise(0).alias(\"PTP\"))\n",
    "        # Accounts by queuedescription\n",
    "        .with_columns(pl.when(pl.col(\"Queuedescription\").is_in([\"COL VB Low Risk Predue -5 to -1\", \"COL VB Med Risk Predue -5 to -4\"])).then(0).otherwise(1).alias(\"HC_Account\"))\n",
    "        .with_columns(pl.when((pl.col(\"Queuedescription\").is_in([\"COL VB Low Risk Predue -5 to -1\", \"COL VB Med Risk Predue -5 to -4\"]))|(pl.col(\"call_time_of_day\") == pl.lit(\"Afterhours\"))).then(1).otherwise(0).alias(\"VB_Account\"))\n",
    "        .with_columns(pl.when(pl.col(\"Queuedescription\").is_in([\"COL VB Low Risk Predue -5 to -1\", \"COL VB Med Risk Predue -5 to -4\"])).then(1).otherwise(0).alias(\"VB_Only_Account\"))\n",
    "        # Count up accounts by dial type\n",
    "        .with_columns((pl.col(\"HC_Account\").sum()\n",
    "                        .over(group_vars) > 0)\n",
    "                        .cast(pl.Int8)\n",
    "                        .alias(\"HC_Account\"))\n",
    "        .with_columns((pl.col(\"VB_Account\").sum()\n",
    "                        .over(group_vars) > 0)\n",
    "                        .cast(pl.Int8)\n",
    "                        .alias(\"VB_Account\"))\n",
    "        .with_columns((pl.col(\"VB_Only_Account\").sum()\n",
    "                        .over(group_vars) > 0)\n",
    "                        .cast(pl.Int8)\n",
    "                        .alias(\"VB_Only_Account\"))\n",
    "        # Spins by dial type\n",
    "        .with_columns(pl.when((pl.col(\"HC_Account\") == 1) & (pl.col(\"call_time_of_day\") == \"Daytime\")).then(1).otherwise(0).alias(\"HC_Spin\"))\n",
    "        .with_columns(pl.when((pl.col(\"HC_Account\") == 1) & (pl.col(\"call_time_of_day\") == \"Daytime\")).then(0).otherwise(1).alias(\"VB_Spin\"))\n",
    "        .sort([\"Loan_ID\", \"call_date\", \"AC_PTP\", \"AC_NO_PTP\", \"AC_Paid_Loan\", \"AC_DNC\", \"AC_Call_Back\", \"AC_Hang_Up\", \"AC_PTP_FFUP\"], descending = [False, False, True, True, True, True, True, True, True])\n",
    "        )\n",
    "        \n",
    "        \n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "55b87222-ebc2-4d82-b47d-42b60b4810ac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def f_CreateSummary(df, timeframe=Literal[\"daily\", \"weekly\", \"monthly\"]):\n",
    "    if timeframe == \"daily\":\n",
    "        group_vars = [\"call_date\", \"DPD_type\", \"Loan_ID\"]\n",
    "    elif timeframe == \"weekly\":\n",
    "        group_vars = [\"week_num\", \"min_date\", \"max_date\", \"week_title\", \"DPD_type\", \"Loan_ID\"]\n",
    "        df = (df\n",
    "              .with_columns(pl.col(\"call_date\").min().over(\"week_num\").alias(\"min_date\"))\n",
    "              .with_columns(pl.col(\"call_date\").max().over(\"week_num\").alias(\"max_date\"))\n",
    "              .with_columns(pl.format(\"Week {}: {} - {}\", \n",
    "                                      pl.col(\"week_num\"), \n",
    "                                      pl.col(\"min_date\").dt.strftime(\"%b-%d\"), \n",
    "                                      pl.col(\"max_date\").dt.strftime(\"%b-%d\"))\n",
    "                            .alias(\"week_title\"))\n",
    "        )\n",
    "    elif timeframe == \"monthly\":\n",
    "        group_vars = [\"call_month\", \"DPD_type\", \"Loan_ID\"]\n",
    "    else:\n",
    "        print(\n",
    "            f\"Invalid timeframe {timeframe} provided. Can only take 'daily', 'weekly', or 'monthly'\"\n",
    "        )\n",
    "\n",
    "    # First create summary that does unique accounts on a hierarchical basis\n",
    "    df_summary = df.group_by(group_vars).agg(\n",
    "        pl.when(pl.col(\"Loan_ID\").n_unique() > 0).then(1).otherwise(0).alias(\"Unique_Accounts\"),\n",
    "        pl.when(pl.col(\"HC_Connect\").sum() > 0).then(1).otherwise(0).alias(\"HC_Connect\"),\n",
    "        pl.when((pl.col(\"HC_Connect\").sum() == 0) & (pl.col(\"IVR_Connect\").sum() > 0)).then(1).otherwise(0).alias(\"IVR_Connect\"),\n",
    "        pl.when((pl.col(\"HC_Connect\").sum() == 0) & (pl.col(\"IVR_Connect\").sum() == 0) & (pl.col(\"VB_Connect\").sum() > 0)).then(1).otherwise(0).alias(\"VB_Connect\"),\n",
    "        pl.when((pl.col(\"HC_Connect\").sum() + pl.col(\"IVR_Connect\").sum() + pl.col(\"VB_Connect\").sum()) > 0).then(1).otherwise(0).alias(\"Total_Connect\"),\n",
    "        pl.when(pl.col(\"HC_Connect_AgentConnect\").sum() > 0).then(1).otherwise(0).alias(\"HC_Connect_AgentConnect\"),\n",
    "        pl.when((pl.col(\"HC_Connect_AgentConnect\").sum() == 0) & (pl.col(\"HC_Connect_DropOff\").sum() > 0)).then(1).otherwise(0).alias(\"HC_Connect_DropOff\"),\n",
    "        pl.when((pl.col(\"HC_Connect_AgentConnect\").sum() == 0) & (pl.col(\"HC_Connect_DropOff\").sum() == 0) & (pl.col(\"HC_Connect_Voicemail\").sum() > 0)).then(1).otherwise(0).alias(\"HC_Connect_Voicemail\"),\n",
    "        pl.when((pl.col(\"HC_Connect_AgentConnect\").sum() == 0) & (pl.col(\"HC_Connect_DropOff\").sum() == 0) & (pl.col(\"HC_Connect_Voicemail\").sum() == 0) & (pl.col(\"HC_Connect_Other\").sum() > 0)).then(1).otherwise(0).alias(\"HC_Connect_Other\"),\n",
    "        pl.when(pl.col(\"AC_PTP\").sum() > 0).then(1).otherwise(0).alias(\"AC_PTP\"),\n",
    "        pl.when((pl.col(\"AC_PTP\").sum() == 0) & (pl.col(\"AC_NO_PTP\").sum() > 0)).then(1).otherwise(0).alias(\"AC_NO_PTP\"),\n",
    "        pl.when((pl.col(\"AC_PTP\").sum() == 0) & (pl.col(\"AC_NO_PTP\").sum() == 0) & (pl.col(\"AC_Paid_Loan\").sum() > 0)).then(1).otherwise(0).alias(\"AC_Paid_Loan\"),\n",
    "        pl.when((pl.col(\"AC_PTP\").sum() == 0) & (pl.col(\"AC_NO_PTP\").sum() == 0) & (pl.col(\"AC_Paid_Loan\").sum() == 0) & (pl.col(\"AC_DNC\").sum() > 0)).then(1).otherwise(0).alias(\"AC_DNC\"),\n",
    "        pl.when((pl.col(\"AC_PTP\").sum() == 0) & (pl.col(\"AC_NO_PTP\").sum() == 0) & (pl.col(\"AC_Paid_Loan\").sum() == 0) & (pl.col(\"AC_DNC\").sum() == 0) & (pl.col(\"AC_Call_Back\").sum() > 0)).then(1).otherwise(0).alias(\"AC_Call_Back\"),\n",
    "        pl.when((pl.col(\"AC_PTP\").sum() == 0) & (pl.col(\"AC_NO_PTP\").sum() == 0) & (pl.col(\"AC_Paid_Loan\").sum() == 0) & (pl.col(\"AC_DNC\").sum() == 0) & (pl.col(\"AC_Call_Back\").sum() == 0) & (pl.col(\"AC_Hang_Up\").sum() > 0)).then(1).otherwise(0).alias(\"AC_Hang_Up\"),\n",
    "        pl.when((pl.col(\"AC_PTP\").sum() == 0) & (pl.col(\"AC_NO_PTP\").sum() == 0) & (pl.col(\"AC_Paid_Loan\").sum() == 0) & (pl.col(\"AC_DNC\").sum() == 0) & (pl.col(\"AC_Call_Back\").sum() == 0) & (pl.col(\"AC_Hang_Up\").sum() == 0) & (pl.col(\"AC_PTP_FFUP\").sum() > 0)).then(1).otherwise(0).alias(\"AC_PTP_FFUP\"),\n",
    "        pl.when(pl.col(\"Processed_account\").sum() > 0).then(1).otherwise(0).alias(\"Processed_account\"),\n",
    "        pl.when(pl.col(\"HC_Account\").sum() > 0).then(1).otherwise(0).alias(\"HC_Account\"),\n",
    "        pl.when(pl.col(\"VB_Account\").sum() > 0).then(1).otherwise(0).alias(\"VB_Account\"),\n",
    "        pl.when(pl.col(\"VB_Only_Account\").sum() > 0).then(1).otherwise(0).alias(\"VB_Only_Account\"),\n",
    "        # Spin basis\n",
    "        pl.col(\"HC_Spin\").sum(),\n",
    "        pl.col(\"VB_Spin\").sum(),\n",
    "        pl.col(\"HC_Connect\").sum().alias(\"HC_Spin_Connect\"),\n",
    "        pl.col(\"IVR_Connect\").sum().alias(\"IVR_Spin_Connect\"),\n",
    "        pl.col(\"VB_Connect\").sum().alias(\"VB_Spin_Connect\"),\n",
    "        pl.col(\"HC_Connect_DropOff\").sum().alias(\"HC_Spin_Connect_DropOff\"),\n",
    "        pl.col(\"HC_Connect_Voicemail\").sum().alias(\"HC_Spin_Connect_Voicemail\"),\n",
    "        pl.col(\"HC_Connect_Other\").sum().alias(\"HC_Spin_Connect_Other\"),\n",
    "        pl.col(\"HC_Connect_AgentConnect\").sum().alias(\"HC_Spin_Connect_AgentConnect\"),\n",
    "        pl.col(\"AC_PTP\").sum().alias(\"AC_Spin_PTP\"),\n",
    "        pl.col(\"AC_NO_PTP\").sum().alias(\"AC_Spin_NO_PTP\"),\n",
    "        pl.col(\"AC_Paid_Loan\").sum().alias(\"AC_Spin_Paid_Loan\"),\n",
    "        pl.col(\"AC_DNC\").sum().alias(\"AC_Spin_DNC\"),\n",
    "        pl.col(\"AC_Call_Back\").sum().alias(\"AC_Spin_Call_Back\"),\n",
    "        pl.col(\"AC_Hang_Up\").sum().alias(\"AC_Spin_Hang_Up\"),\n",
    "        pl.col(\"AC_PTP_FFUP\").sum().alias(\"AC_Spin_PTP_FFUP\")\n",
    "    )\n",
    "\n",
    "    # Summarise the unique account based summary into Tableau summary\n",
    "    group_vars = [x for x in group_vars if x != \"Loan_ID\"]\n",
    "    \n",
    "    df_summary = df_summary.group_by(group_vars).agg(\n",
    "        pl.col(\"Unique_Accounts\").sum(),\n",
    "        pl.col(\"HC_Connect\").sum(),\n",
    "        pl.col(\"IVR_Connect\").sum(),\n",
    "        pl.col(\"VB_Connect\").sum(),\n",
    "        pl.col(\"Total_Connect\").sum(),\n",
    "        pl.col(\"HC_Connect_AgentConnect\").sum(),\n",
    "        pl.col(\"HC_Connect_DropOff\").sum(),\n",
    "        pl.col(\"HC_Connect_Voicemail\").sum(),\n",
    "        pl.col(\"HC_Connect_Other\").sum(),\n",
    "        pl.col(\"AC_PTP\").sum(),\n",
    "        pl.col(\"AC_NO_PTP\").sum(),\n",
    "        pl.col(\"AC_Paid_Loan\").sum(),\n",
    "        pl.col(\"AC_DNC\").sum(),\n",
    "        pl.col(\"AC_Call_Back\").sum(),\n",
    "        pl.col(\"AC_Hang_Up\").sum(),\n",
    "        pl.col(\"AC_PTP_FFUP\").sum(),\n",
    "        pl.col(\"Processed_account\").sum(),\n",
    "        pl.col(\"HC_Account\").sum(),\n",
    "        pl.col(\"VB_Account\").sum(),\n",
    "        pl.col(\"HC_Spin\").sum(),\n",
    "        pl.col(\"VB_Spin\").sum(),\n",
    "        pl.col(\"HC_Spin_Connect\").sum(),\n",
    "        pl.col(\"IVR_Spin_Connect\").sum(),\n",
    "        pl.col(\"VB_Spin_Connect\").sum(),\n",
    "        pl.col(\"HC_Spin_Connect_DropOff\").sum(),\n",
    "        pl.col(\"HC_Spin_Connect_Voicemail\").sum(),\n",
    "        pl.col(\"HC_Spin_Connect_Other\").sum(),\n",
    "        pl.col(\"HC_Spin_Connect_AgentConnect\").sum(),\n",
    "        pl.col(\"AC_Spin_PTP\").sum(),\n",
    "        pl.col(\"AC_Spin_NO_PTP\").sum(),\n",
    "        pl.col(\"AC_Spin_Paid_Loan\").sum(),\n",
    "        pl.col(\"AC_Spin_DNC\").sum(),\n",
    "        pl.col(\"AC_Spin_Call_Back\").sum(),\n",
    "        pl.col(\"AC_Spin_Hang_Up\").sum(),\n",
    "        pl.col(\"AC_Spin_PTP_FFUP\").sum(),\n",
    "        pl.col(\"VB_Only_Account\").sum(),\n",
    "    )\n",
    "\n",
    "    return df_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fc0430ea-1a21-4827-9f50-59bc564849f9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Daily functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "a8982280-cb6c-4c7a-b29e-d2f03887cda3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def f_DailyCreateSparkTable(df, table_name:str):\n",
    "    # Convert to PySpark Dataframe\n",
    "    from pyspark.sql.types import (\n",
    "        StructType, StructField,\n",
    "        DateType, StringType, IntegerType, LongType\n",
    "    )\n",
    "\n",
    "    # Ensure that the order of the schema columns are the same as the column ordering in the dataframe\n",
    "    schema = StructType([\n",
    "        StructField(\"call_date\", DateType(), True),\n",
    "        StructField(\"DPD_type\", StringType(), True),\n",
    "        StructField(\"Unique_Accounts\", IntegerType(), True),\n",
    "        StructField(\"HC_Connect\", IntegerType(), True),\n",
    "        StructField(\"IVR_Connect\", IntegerType(), True),\n",
    "        StructField(\"VB_Connect\", IntegerType(), True),\n",
    "        StructField(\"Total_Connect\", IntegerType(), True),\n",
    "        StructField(\"HC_Connect_AgentConnect\", IntegerType(), True),\n",
    "        StructField(\"HC_Connect_DropOff\", IntegerType(), True),\n",
    "        StructField(\"HC_Connect_Voicemail\", IntegerType(), True),\n",
    "        StructField(\"HC_Connect_Other\", IntegerType(), True),\n",
    "        StructField(\"AC_PTP\", IntegerType(), True),\n",
    "        StructField(\"AC_NO_PTP\", IntegerType(), True),\n",
    "        StructField(\"AC_Paid_Loan\", IntegerType(), True),\n",
    "        StructField(\"AC_DNC\", IntegerType(), True),\n",
    "        StructField(\"AC_Call_Back\", IntegerType(), True),\n",
    "        StructField(\"AC_Hang_Up\", IntegerType(), True),\n",
    "        StructField(\"AC_PTP_FFUP\", IntegerType(), True),\n",
    "        StructField(\"Processed_account\", IntegerType(), True),\n",
    "        StructField(\"HC_Account\", IntegerType(), True),\n",
    "        StructField(\"VB_Account\", IntegerType(), True),\n",
    "        StructField(\"HC_Spin\", IntegerType(), True),\n",
    "        StructField(\"VB_Spin\", IntegerType(), True),\n",
    "        StructField(\"HC_Spin_Connect\", IntegerType(), True),\n",
    "        StructField(\"IVR_Spin_Connect\", IntegerType(), True),\n",
    "        StructField(\"VB_Spin_Connect\", IntegerType(), True),\n",
    "        StructField(\"HC_Spin_Connect_DropOff\", IntegerType(), True),\n",
    "        StructField(\"HC_Spin_Connect_Voicemail\", IntegerType(), True),\n",
    "        StructField(\"HC_Spin_Connect_Other\", IntegerType(), True),\n",
    "        StructField(\"HC_Spin_Connect_AgentConnect\", IntegerType(), True),\n",
    "        StructField(\"AC_Spin_PTP\", IntegerType(), True),\n",
    "        StructField(\"AC_Spin_NO_PTP\", IntegerType(), True),\n",
    "        StructField(\"AC_Spin_Paid_Loan\", IntegerType(), True),\n",
    "        StructField(\"AC_Spin_DNC\", IntegerType(), True),\n",
    "        StructField(\"AC_Spin_Call_Back\", IntegerType(), True),\n",
    "        StructField(\"AC_Spin_Hang_Up\", IntegerType(), True),\n",
    "        StructField(\"AC_Spin_PTP_FFUP\", IntegerType(), True),\n",
    "        StructField(\"VB_Only_Account\", IntegerType(), True),\n",
    "    ])\n",
    "\n",
    "    spark.sql(f\"DROP TABLE IF EXISTS {table_name}\")\n",
    "    spark_df = spark.createDataFrame(df, schema=schema)\n",
    "\n",
    "    return spark_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "40e5a26f-09ba-4229-bf5a-2cfa8061116c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Weekly functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "110a53f2-e2b3-4ed1-9f71-dd4ceadd55ca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def f_WeeklyCreateWeekNum(df):\n",
    "    df = (\n",
    "        df\n",
    "        # Group Queuedescription into pre-due and M1 buckets\n",
    "        .with_columns(pl.when(pl.col(\"Queuedescription\").str.contains(\"M1\"))\n",
    "                      .then(pl.lit(\"M1\"))\n",
    "                      .otherwise(pl.lit(\"Pre-Due\"))\n",
    "                      .alias(\"DPD_type\"))\n",
    "        # Convert call time into a date and time column\n",
    "        .with_columns((pl.col(\"Incomingcalltime\").cast(pl.Float64).map_elements(lambda x: excel_epoch + timedelta(days=x) if x is not None else None).alias(\"incoming_calltime_converted\")))\n",
    "        .with_columns(pl.col(\"incoming_calltime_converted\").map_elements(lambda x: x.date() if x is not None else None).alias(\"call_date\"))\n",
    "        .with_columns(pl.col(\"incoming_calltime_converted\").map_elements(lambda x: x.time() if x is not None else None).alias(\"call_time\"))\n",
    "        .with_columns(pl.col(\"IVR_Time\").str.strptime(pl.Time, \"%H:%M:%S\").alias(\"ivr_time_converted\"))\n",
    "        # Bucket call types into Daytime (8am - 5pm) and Afterhours (5pm - 9pm\n",
    "        .with_columns(pl.when((pl.col(\"call_time\") >= time(8, 0)) & (pl.col(\"call_time\") < time(17, 0)))\n",
    "                      .then(pl.lit(\"Daytime\"))\n",
    "                      .otherwise(pl.lit(\"Afterhours\"))\n",
    "                      .alias(\"call_time_of_day\"))\n",
    "        # Create Nth week of year column\n",
    "        .with_columns(pl.col(\"call_date\").dt.week().alias(\"week_num\"))\n",
    "        )\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "3ae4853a-1f14-4a88-9762-87c5c14374e3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def f_WeeklyCreateSparkTable(df, table_name:str):\n",
    "    # Convert to PySpark Dataframe\n",
    "    from pyspark.sql.types import (\n",
    "        StructType, StructField,\n",
    "        DateType, StringType, IntegerType, LongType\n",
    "        )\n",
    "\n",
    "    # Ensure that the order of the schema columns are the same as the column ordering in the dataframe\n",
    "    schema = StructType([\n",
    "        StructField(\"week_num\", IntegerType(), True),\n",
    "        StructField(\"min_date\", DateType(), True),\n",
    "        StructField(\"max_date\", DateType(), True),\n",
    "        StructField(\"week_title\", StringType(), True),\n",
    "        StructField(\"DPD_type\", StringType(), True),\n",
    "        StructField(\"Unique_Accounts\", IntegerType(), True),\n",
    "        StructField(\"HC_Connect\", IntegerType(), True),\n",
    "        StructField(\"IVR_Connect\", IntegerType(), True),\n",
    "        StructField(\"VB_Connect\", IntegerType(), True),\n",
    "        StructField(\"Total_Connect\", IntegerType(), True),\n",
    "        StructField(\"HC_Connect_AgentConnect\", IntegerType(), True),\n",
    "        StructField(\"HC_Connect_DropOff\", IntegerType(), True),\n",
    "        StructField(\"HC_Connect_Voicemail\", IntegerType(), True),\n",
    "        StructField(\"HC_Connect_Other\", IntegerType(), True),\n",
    "        StructField(\"AC_PTP\", IntegerType(), True),\n",
    "        StructField(\"AC_NO_PTP\", IntegerType(), True),\n",
    "        StructField(\"AC_Paid_Loan\", IntegerType(), True),\n",
    "        StructField(\"AC_DNC\", IntegerType(), True),\n",
    "        StructField(\"AC_Call_Back\", IntegerType(), True),\n",
    "        StructField(\"AC_Hang_Up\", IntegerType(), True),\n",
    "        StructField(\"AC_PTP_FFUP\", IntegerType(), True),\n",
    "        StructField(\"Processed_account\", IntegerType(), True),\n",
    "        StructField(\"HC_Account\", IntegerType(), True),\n",
    "        StructField(\"VB_Account\", IntegerType(), True),\n",
    "        StructField(\"HC_Spin\", IntegerType(), True),\n",
    "        StructField(\"VB_Spin\", IntegerType(), True),\n",
    "        StructField(\"HC_Spin_Connect\", IntegerType(), True),\n",
    "        StructField(\"IVR_Spin_Connect\", IntegerType(), True),\n",
    "        StructField(\"VB_Spin_Connect\", IntegerType(), True),\n",
    "        StructField(\"HC_Spin_Connect_DropOff\", IntegerType(), True),\n",
    "        StructField(\"HC_Spin_Connect_Voicemail\", IntegerType(), True),\n",
    "        StructField(\"HC_Spin_Connect_Other\", IntegerType(), True),\n",
    "        StructField(\"HC_Spin_Connect_AgentConnect\", IntegerType(), True),\n",
    "        StructField(\"AC_Spin_PTP\", IntegerType(), True),\n",
    "        StructField(\"AC_Spin_NO_PTP\", IntegerType(), True),\n",
    "        StructField(\"AC_Spin_Paid_Loan\", IntegerType(), True),\n",
    "        StructField(\"AC_Spin_DNC\", IntegerType(), True),\n",
    "        StructField(\"AC_Spin_Call_Back\", IntegerType(), True),\n",
    "        StructField(\"AC_Spin_Hang_Up\", IntegerType(), True),\n",
    "        StructField(\"AC_Spin_PTP_FFUP\", IntegerType(), True),\n",
    "        StructField(\"VB_Only_Account\", IntegerType(), True),\n",
    "        ])  \n",
    "\n",
    "    spark.sql(f\"DROP TABLE IF EXISTS {table_name}\")\n",
    "    spark_df = spark.createDataFrame(df, schema=schema)\n",
    "\n",
    "    return spark_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3340c51b-a058-4206-8cbd-93856ae2d427",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Monthly functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4e5013ec-5f6b-4b95-8259-231a7c34fde4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def f_MonthlyCreateMonthNum(df):\n",
    "    df = (\n",
    "        df\n",
    "        # Group Queuedescription into pre-due and M1 buckets\n",
    "        .with_columns(pl.when(pl.col(\"Queuedescription\").str.contains(\"M1\"))\n",
    "                      .then(pl.lit(\"M1\"))\n",
    "                      .otherwise(pl.lit(\"Pre-Due\"))\n",
    "                      .alias(\"DPD_type\"))\n",
    "        # Convert call time into a date and time column\n",
    "        .with_columns((pl.col(\"Incomingcalltime\").cast(pl.Float64).map_elements(lambda x: excel_epoch + timedelta(days=x) if x is not None else None).alias(\"incoming_calltime_converted\")))\n",
    "        .with_columns(pl.col(\"incoming_calltime_converted\").map_elements(lambda x: x.date() if x is not None else None).alias(\"call_date\"))\n",
    "        .with_columns(pl.col(\"incoming_calltime_converted\").map_elements(lambda x: x.time() if x is not None else None).alias(\"call_time\"))\n",
    "        .with_columns(pl.col(\"IVR_Time\").str.strptime(pl.Time, \"%H:%M:%S\").alias(\"ivr_time_converted\"))\n",
    "        # Bucket call types into Daytime (8am - 5pm) and Afterhours (5pm - 9pm\n",
    "        .with_columns(pl.when((pl.col(\"call_time\") >= time(8, 0)) & (pl.col(\"call_time\") < time(17, 0)))\n",
    "                      .then(pl.lit(\"Daytime\"))\n",
    "                      .otherwise(pl.lit(\"Afterhours\"))\n",
    "                      .alias(\"call_time_of_day\"))\n",
    "        # Floor call_date to start of month\n",
    "        .with_columns(pl.col(\"call_date\").dt.month_start().alias(\"call_month\"))\n",
    "    )\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d4ee272f-3997-4448-96dc-4231c1b10f13",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def f_MonthlyCreateSparkTable(df, table_name:str):\n",
    "    # Convert to PySpark Dataframe\n",
    "    from pyspark.sql.types import (\n",
    "        StructType, StructField,\n",
    "        DateType, StringType, IntegerType, LongType\n",
    "        )\n",
    "\n",
    "    # Ensure that the order of the schema columns are the same as the column ordering in the dataframe\n",
    "    schema = StructType([\n",
    "        StructField(\"call_month\", DateType(), True),\n",
    "        StructField(\"DPD_type\", StringType(), True),\n",
    "        StructField(\"Unique_Accounts\", IntegerType(), True),\n",
    "        StructField(\"HC_Connect\", IntegerType(), True),\n",
    "        StructField(\"IVR_Connect\", IntegerType(), True),\n",
    "        StructField(\"VB_Connect\", IntegerType(), True),\n",
    "        StructField(\"Total_Connect\", IntegerType(), True),\n",
    "        StructField(\"HC_Connect_AgentConnect\", IntegerType(), True),\n",
    "        StructField(\"HC_Connect_DropOff\", IntegerType(), True),\n",
    "        StructField(\"HC_Connect_Voicemail\", IntegerType(), True),\n",
    "        StructField(\"HC_Connect_Other\", IntegerType(), True),\n",
    "        StructField(\"AC_PTP\", IntegerType(), True),\n",
    "        StructField(\"AC_NO_PTP\", IntegerType(), True),\n",
    "        StructField(\"AC_Paid_Loan\", IntegerType(), True),\n",
    "        StructField(\"AC_DNC\", IntegerType(), True),\n",
    "        StructField(\"AC_Call_Back\", IntegerType(), True),\n",
    "        StructField(\"AC_Hang_Up\", IntegerType(), True),\n",
    "        StructField(\"AC_PTP_FFUP\", IntegerType(), True),\n",
    "        StructField(\"Processed_account\", IntegerType(), True),\n",
    "        StructField(\"HC_Account\", IntegerType(), True),\n",
    "        StructField(\"VB_Account\", IntegerType(), True),\n",
    "        StructField(\"HC_Spin\", IntegerType(), True),\n",
    "        StructField(\"VB_Spin\", IntegerType(), True),\n",
    "        StructField(\"HC_Spin_Connect\", IntegerType(), True),\n",
    "        StructField(\"IVR_Spin_Connect\", IntegerType(), True),\n",
    "        StructField(\"VB_Spin_Connect\", IntegerType(), True),\n",
    "        StructField(\"HC_Spin_Connect_DropOff\", IntegerType(), True),\n",
    "        StructField(\"HC_Spin_Connect_Voicemail\", IntegerType(), True),\n",
    "        StructField(\"HC_Spin_Connect_Other\", IntegerType(), True),\n",
    "        StructField(\"HC_Spin_Connect_AgentConnect\", IntegerType(), True),\n",
    "        StructField(\"AC_Spin_PTP\", IntegerType(), True),\n",
    "        StructField(\"AC_Spin_NO_PTP\", IntegerType(), True),\n",
    "        StructField(\"AC_Spin_Paid_Loan\", IntegerType(), True),\n",
    "        StructField(\"AC_Spin_DNC\", IntegerType(), True),\n",
    "        StructField(\"AC_Spin_Call_Back\", IntegerType(), True),\n",
    "        StructField(\"AC_Spin_Hang_Up\", IntegerType(), True),\n",
    "        StructField(\"AC_Spin_PTP_FFUP\", IntegerType(), True),\n",
    "        StructField(\"VB_Only_Account\", IntegerType(), True),\n",
    "        ])  \n",
    "\n",
    "    spark.sql(f\"DROP TABLE IF EXISTS {table_name}\")\n",
    "    spark_df = spark.createDataFrame(df, schema=schema)\n",
    "\n",
    "    return spark_df"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "(Clone) my_packages",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}